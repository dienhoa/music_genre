{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34fbcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import torchaudio\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import librosa\n",
    "import kornia\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7653af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07cdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('/home/ubuntu/.kaggle/kaggle-pog-series-s01e02/train.csv')\n",
    "df_test = pd.read_csv('/home/ubuntu/.kaggle/kaggle-pog-series-s01e02/test.csv')\n",
    "submission = pd.read_csv('/home/ubuntu/.kaggle/kaggle-pog-series-s01e02/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963abb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/')\n",
    "test_path = Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/')\n",
    "# train_path = Path('/home/ubuntu/.kaggle/rescale/spectograms/train/')\n",
    "# test_path = Path('/home/ubuntu/.kaggle/rescale/spectograms/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ba9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = get_image_files(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6efd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#19909) [Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/006638.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/000618.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/016641.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/023943.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/023478.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/016103.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/005110.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/023831.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/000200.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/007218.png')...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566ebff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(filename):\n",
    "    resample_name = filename.stem + '.ogg'\n",
    "    return df_train[df_train['filename']==resample_name]['genre'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bb6cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluded unusual music thanks to this thread: https://www.kaggle.com/c/kaggle-pog-series-s01e02/discussion/312842\n",
    "def get_items(path): \n",
    "    excluded_files = [\"010449.png\" , \n",
    "                      \"005589.png\" , \n",
    "                      \"004921.png\", \n",
    "                      \"019511.png\" , \n",
    "                      \"013375.png\" , \n",
    "                      \"024247.png\", \n",
    "                      \"024156.png\"]\n",
    "    items = get_image_files(path)\n",
    "    items = [item for item in items if item.name not in excluded_files]\n",
    "    \n",
    "    ## For fast iteration\n",
    "#     items = [item for item in items if get_y(item) in ['Punk', 'Rock']]\n",
    "    random.shuffle(items)\n",
    "#     items.shuffle()\n",
    "    return L(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5889a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_FFT = 2048\n",
    "# HOP_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1444cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_spectrogram(filename):\n",
    "#     audio, sr = torchaudio.load(filename)\n",
    "    \n",
    "#     audio = audio.squeeze()\n",
    "#     audio, index = librosa.effects.trim(audio, frame_length=1024, hop_length=256)  \n",
    "#     audio = audio.unsqueeze(0)\n",
    "        \n",
    "#     specgram = torchaudio.transforms.MelSpectrogram(sample_rate=sr, \n",
    "#                                                     n_fft=N_FFT, \n",
    "#                                                     win_length=N_FFT, \n",
    "#                                                     hop_length=HOP_LEN,\n",
    "#                                                     center=True,\n",
    "#                                                     pad_mode=\"reflect\",\n",
    "#                                                     power=2.0,\n",
    "#                                                     norm='slaney',\n",
    "#                                                     onesided=True,\n",
    "#                                                     n_mels=128,\n",
    "#                                                     mel_scale=\"htk\"\n",
    "#                                                    )(audio)[0]\n",
    "#     specgram = torchaudio.transforms.AmplitudeToDB()(specgram)\n",
    "#     specgram = specgram - specgram.min()\n",
    "#     specgram = specgram/specgram.max()*255\n",
    "    \n",
    "    \n",
    "#     return specgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58465de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = train_files[0]\n",
    "# spec_default = create_spectrogram(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb0d48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = get_items(train_path)\n",
    "labels = [get_y(item) for item in items]\n",
    "count = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "568ce4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_items = get_items(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ede7502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5076) [Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/023435.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/022501.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/005887.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/007003.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/005355.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/008545.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/002576.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/001819.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/021452.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/test/013064.png')...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98cfa172",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread(test_items[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3d8c2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 1293, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b9ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter = TrainTestSplitter(test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627abd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cf07246",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_kfold=[]\n",
    "for _, val_idx in kf.split(np.array(items), labels):\n",
    "    splits = IndexSplitter(val_idx)\n",
    "    split = splits(items)\n",
    "    split_list = [split[0], split[1]]\n",
    "    splits_kfold.append(split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ef646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(#17058) [0,1,2,4,5,6,8,9,10,11...],\n",
       "  (#2844) [3,7,24,27,32,33,36,37,39,40...]],\n",
       " [(#17059) [1,2,3,5,6,7,8,9,10,11...],\n",
       "  (#2843) [0,4,13,41,48,63,83,89,91,98...]],\n",
       " [(#17059) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2843) [15,17,18,21,22,26,29,46,50,51...]],\n",
       " [(#17059) [0,3,4,5,7,10,11,12,13,14...],\n",
       "  (#2843) [1,2,6,8,9,23,35,44,45,47...]],\n",
       " [(#17059) [0,1,2,3,4,6,7,8,9,11...],\n",
       "  (#2843) [5,10,12,14,16,25,34,53,69,71...]],\n",
       " [(#17059) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2843) [11,20,30,42,52,61,66,74,90,92...]],\n",
       " [(#17059) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2843) [19,28,31,38,65,104,123,125,126,134...]]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8958945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_learner(learn, channels=1):\n",
    "    learn.model[0][0][0].in_channels=channels\n",
    "    learn.model[0][0][0].weight = torch.nn.parameter.Parameter(learn.model[0][0][0].weight[:,1,:,:].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8aaea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genreid_from_genre(genre):\n",
    "    return int(genre2id[genre2id['genre'] == genre]['genre_id'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e02417",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_1fold = splits_kfold[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2780b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(#17058) [0,1,2,4,5,6,8,9,10,11...],\n",
       "  (#2844) [3,7,24,27,32,33,36,37,39,40...]],\n",
       " [(#17059) [1,2,3,5,6,7,8,9,10,11...],\n",
       "  (#2843) [0,4,13,41,48,63,83,89,91,98...]],\n",
       " [(#17059) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2843) [15,17,18,21,22,26,29,46,50,51...]],\n",
       " [(#17059) [0,3,4,5,7,10,11,12,13,14...],\n",
       "  (#2843) [1,2,6,8,9,23,35,44,45,47...]],\n",
       " [(#17059) [0,1,2,3,4,6,7,8,9,11...],\n",
       "  (#2843) [5,10,12,14,16,25,34,53,69,71...]],\n",
       " [(#17059) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2843) [11,20,30,42,52,61,66,74,90,92...]],\n",
       " [(#17059) [0,1,2,3,4,5,6,7,8,9...],\n",
       "  (#2843) [19,28,31,38,65,104,123,125,126,134...]]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594e433",
   "metadata": {},
   "source": [
    "### Test Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "926d1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = untar_data(URLs.PETS)\n",
    "\n",
    "# test_imgs = get_image_files(path)\n",
    "\n",
    "# TEST_IMAGE = test_imgs[4]\n",
    "\n",
    "# img = PILImage(PILImage.create(TEST_IMAGE))\n",
    "\n",
    "\n",
    "class ReflectionCrop(RandomCrop):\n",
    "    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n",
    "        return x.crop_pad(self.size, self.tl, orig_sz=self.orig_sz, pad_mode=PadMode.Reflection)\n",
    "\n",
    "# rsz = ReflectionCrop((375, 100))\n",
    "\n",
    "# img2 = rsz(img, split_idx=0)\n",
    "\n",
    "# img.shape\n",
    "\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ab39f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#19902) [Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/018703.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/008986.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/008092.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/002736.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/010659.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/023736.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/012810.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/004398.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/022141.png'),Path('/home/ubuntu/.kaggle/spectrograms/spectograms/train/000430.png')...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "799fc3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(#17058) [0,1,2,4,5,6,8,9,10,11...], (#2844) [3,7,24,27,32,33,36,37,39,40...]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits_1fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae0d8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataBlock(DataBlock):\n",
    "    def datasets(self:DataBlock, source, verbose=False, splits=None):\n",
    "        self.source = source                     ; pv(f\"Collecting items from {source}\", verbose)\n",
    "        items = (self.get_items or noop)(source) ; pv(f\"Found {len(items)} items\", verbose)\n",
    "        pv(f\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\", verbose)\n",
    "        return Datasets(items, tfms=self._combine_type_tfms(), splits=splits, dl_type=self.dl_type, n_inp=self.n_inp, verbose=verbose)\n",
    "    def dataloaders(self, source, path='.', verbose=False, splits=None, **kwargs):\n",
    "        dsets = self.datasets(source, verbose=verbose, splits=splits)\n",
    "        kwargs = {**self.dls_kwargs, **kwargs, 'verbose': verbose}\n",
    "        return dsets.dataloaders(path=path, after_item=self.item_tfms, after_batch=self.batch_tfms, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13cc5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_MP_to_blurMP(model, layer_type_old):\n",
    "    conversion_count = 0\n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            # recurse\n",
    "            model._modules[name] = convert_MP_to_blurMP(module, layer_type_old)\n",
    "\n",
    "        if type(module) == layer_type_old:\n",
    "            layer_old = module\n",
    "            layer_new = kornia.contrib.MaxBlurPool2d(3, True)\n",
    "            model._modules[name] = layer_new\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05ff73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(splits_1fold, nb_epoch=45):\n",
    "    \n",
    "    db = CustomDataBlock(\n",
    "        blocks=(ImageBlock, CategoryBlock),\n",
    "        get_y=get_y,\n",
    "        item_tfms=[ReflectionCrop(224)],\n",
    "        batch_tfms=aug_transforms(max_rotate=0, max_warp=0,size=224))\n",
    "    \n",
    "    dls = db.dataloaders(items, splits=splits_1fold)\n",
    "    \n",
    "    mixup = MixUp(0.4)\n",
    "    model = xse_resnext50(n_out=19, act_cls=Mish, sa=1, pool=MaxPool, pretrained=False)\n",
    "    model = convert_MP_to_blurMP(model, nn.MaxPool2d)\n",
    "    \n",
    "    learn = Learner(dls, \n",
    "                    model, \n",
    "                    metrics=[accuracy, F1Score(average='micro')], \n",
    "                    loss_func=LabelSmoothingCrossEntropy(), \n",
    "                    opt_func=ranger, \n",
    "                   )\n",
    "    \n",
    "    learn.to_fp16()\n",
    "    \n",
    "#     nchannels = 1\n",
    "#     alter_learner(learn, nchannels)\n",
    "    \n",
    "    learn.fit_flat_cos(nb_epoch, 0.002, cbs=[mixup, CSVLogger(fname=f'history_{int(time.time())}.csv'), ShowGraphCallback()])\n",
    "    \n",
    "#     test_dl = dls.test_dl(test_items)\n",
    "#     preds = learn.get_preds(dl=test_dl)\n",
    "    return None, learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a9baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.22% [1/45 03:23<2:29:22]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.271588</td>\n",
       "      <td>2.038288</td>\n",
       "      <td>0.419480</td>\n",
       "      <td>0.419480</td>\n",
       "      <td>03:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='30' class='' max='266' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      11.28% [30/266 00:22<02:53 2.2657]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_kfold_learns = []\n",
    "for split_1fold in splits_kfold[:2]:\n",
    "    preds_1fold_learns = get_preds(split_1fold)\n",
    "    preds_kfold_learns.append(preds_1fold_learns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2957b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(preds_kfold_learns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad120c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, learn = preds_kfold_learns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dl = learn.dls.test_dl(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = learn.get_preds(dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "_before_epoch = [event.before_fit, event.before_epoch]\n",
    "_after_epoch  = [event.after_epoch, event.after_fit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def ttacustom(self:Learner, ds_idx=1, dl=None, n=4, item_tfms=None, batch_tfms=None, beta=0.25, use_max=False):\n",
    "    \"Return predictions on the `ds_idx` dataset or `dl` using Test Time Augmentation\"\n",
    "    if dl is None: dl = self.dls[ds_idx].new(shuffled=False, drop_last=False)\n",
    "    if item_tfms is not None or batch_tfms is not None: dl = dl.new(after_item=item_tfms, after_batch=batch_tfms)\n",
    "    try:\n",
    "        self(_before_epoch)\n",
    "        with dl.dataset.set_split_idx(0), self.no_mbar():\n",
    "            if hasattr(self,'progress'): self.progress.mbar = master_bar(list(range(n)))\n",
    "            aug_preds = []\n",
    "            for i in self.progress.mbar if hasattr(self,'progress') else range(n):\n",
    "                self.epoch = i #To keep track of progress on mbar since the progress callback will use self.epoch\n",
    "                preds = self.get_preds(dl=dl, inner=True)[0][None]\n",
    "                preds_idx = preds.squeeze().argmax(1)\n",
    "                aug_preds.append(preds_idx)\n",
    "#         aug_preds = torch.cat(aug_preds)\n",
    "#         aug_preds = aug_preds.max(0)[0] if use_max else aug_preds.mean(0)\n",
    "#         self.epoch = n\n",
    "#         with dl.dataset.set_split_idx(1): preds,targs = self.get_preds(dl=dl, inner=True)\n",
    "    finally: self(event.after_fit)\n",
    "\n",
    "#     if use_max: return torch.stack([preds, aug_preds], 0).max(0)[0],targs\n",
    "#     preds = (aug_preds,preds) if beta is None else torch.lerp(aug_preds, preds, beta)\n",
    "    return aug_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a71dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_preds = learn.ttacustom(dl=test_dl, n=2, beta=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52205b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487430a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e357c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_kfold = [pred_learn[0] for pred_learn in preds_kfold_learns]\n",
    "learns = [pred_learn[1] for pred_learn in preds_kfold_learns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f03b66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: ReflectionCrop -- {'size': (224, 224), 'p': 1.0} -> ToTensor"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.dls.after_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a7c2895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} -> Flip -- {'size': 224, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5} -> Brightness -- {'max_lighting': 0.2, 'p': 1.0, 'draw': None, 'batch': False}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.dls.after_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "IntToFloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe21e2fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aug_preds = []\n",
    "for learn in learns:\n",
    "    test_dl = learn.dls.test_dl(test_items)\n",
    "    aug_preds_1fold = learn.ttacustom(dl=test_dl, n=50, beta=None, item_tfms=learn.dls.after_item, batch_tfms=IntToFloatTensor)\n",
    "    aug_preds.extend(aug_preds_1fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_kfold = [pred_learn[0][0] for pred_learn in preds_kfold_learns]\n",
    "# preds_list = [pred.argmax(axis=1) for pred in preds_kfold]\n",
    "# preds_array = np.array(preds_list)\n",
    "# final_votes = preds_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb28887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9588b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "000e4265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aug_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "100a4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_votes = stats.mode(torch.vstack(aug_preds))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bdd9ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_kfold = [pred_learn[0] for pred_learn in preds_kfold_learns]\n",
    "# preds_kfold = [np.array(preds[0]) for preds in preds_kfold]\n",
    "# preds_list = [pred.argmax(axis=1) for pred in preds_kfold]\n",
    "# preds_array = np.array(preds_list)\n",
    "# values, counts = np.unique(preds_array,axis=0, return_counts=True)\n",
    "# Counter(preds_array[:,0]).most_common(1)[0][0]\n",
    "# final_votes = [Counter(preds_array[:,i]).most_common(1)[0][0] for i in range(preds_array.shape[1])]\n",
    "# final_preds = np.array(preds_kfold).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc4ccbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_votes = preds_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe480017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ae80d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission[submission['song_id']==\"024013\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f702f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5076])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vstack(aug_preds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "70863dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5076,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_votes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e8ddc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_idx = final_preds.argmax(axis=1)\n",
    "genre2id = pd.read_csv('/home/ubuntu/.kaggle/kaggle-pog-series-s01e02/genres.csv')\n",
    "songid_preds = {int(file_path.stem):genreid_from_genre(learns[0].dls.vocab[_id]) for file_path, _id in zip(test_items,final_votes)}\n",
    "submission['genre_id'] = submission['song_id'].map(songid_preds)\n",
    "submission['genre_id'].fillna(0, inplace=True)\n",
    "submission.loc[submission['song_id']==22612, 'genre_id'] = 1\n",
    "submission.loc[submission['song_id']==24013, 'genre_id'] = 0\n",
    "\n",
    "submission.genre_id = submission.genre_id.astype(int)\n",
    "submission.to_csv(f\"submission_final_{int(time.time())}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ad546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab5cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
